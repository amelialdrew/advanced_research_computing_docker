{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JamesFergusson/Introduction-to-Research-Computing/blob/master/09_Parallelisation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelisation\n",
    "\n",
    "Sometimes projects you work on will be too large or slow for even a highly optimised code to run on your desktop/laptop.  In this case the only way to get performance benefits is to write the code to take advantage of multiple CPUs. To do this can be tricky and is something you should consider in the prototyping stage as often serial code can prove impossible to parallelise efficiently without wholesale changes.\n",
    "\n",
    "Parallelisation is becoming more important as since early 2000 serial performance of CPU's has not improved but the parallel performance has increased by 32x per core. This is the only way we are keeping up with Moores law and currently cutting edge CPUs are being built with 70+ cores which will filter down to normal computing soon so parallelisation is becoming more and more important.\n",
    "\n",
    "![](Plots/CPUClock.png)\n",
    "\n",
    "The parallelisation model you settle on for your code should also take into account of the likely computer architecture you want to run on and you will need to consider tradeoffs between memory constraints and communication constraints.  In this section we will have a look at some of the issues we face when we try to think parallel.\n",
    "\n",
    "## Parallelisation levels\n",
    "\n",
    "There are multiple levels in which you can parallelise your code\n",
    "\n",
    "0. Multiple serial jobs. The first option for any code as it has perfect scaling and almost no overhead\n",
    "1. CPU level parallelism like SIMD (single instruction multiple data) or Vectorisation. Works on single CPU, mostly done by compilers\n",
    "2. Multicore or Thread based parallelism. Works on multiple CPU's on the same machine\n",
    "3. Distributed Computing or Process based parallelism.  Uses multiple machines/nodes\n",
    "\n",
    "The 0th order approach is to just run multiple instances of your serial code. This works best when scanning parameter space or processing multiple data files.  You just break your inputs into blocks and run many simultaneous jobs each processing their own block.  If you can do this, do it.  It is simple, scales perfectly and is trivial to implement.  You will never write parallel code that out-performs this solution.\n",
    "\n",
    "The 1st, vectorisation, is hard to control in python. You mainly access it by using optimised libraries which are written in a compiled code like C where there are lots of things you can do, this is why you should try to use numpy and scipy for calculations.  An example of vectorisation is when you apply an operation to a array.  The array can then be read into memory in blocks, which match the size of the CPU register, and operated on simultaneously with the same instruction.  If the register can hold 8 floating point numbers then you get a speed up of a factor 8 (in theory).\n",
    "\n",
    "The 2nd, thread based parallelism, can be done in python with the `threading` or `multiprocessing` packages.  This creates multiple threads which share the same data and can all operate on it.  Unfortunately this is usually pointless due to the Global Interpreter Lock (GIL).  The GIL blocks more than one thread from accessing the interpreter at a time so only one thread can do anything at one time.  As a result threaded code in python seldom runs any faster (there are some exceptions).  This is a general problem with python in that as it is designed to be interactive it is inherently opposed to parallelism.  Numpy does (sometimes) release the GIL so you can get a benefit with multi-threads, otherwise see https://scipy-cookbook.readthedocs.io/items/ParallelProgramming.html.  We will revisit this when we come to Cython which supports `OpenMP` which is the industry standard for thread parallelism and here we can do it much more easily.  You can however utilise packages which support threading as they are compiled in C so avoid the GIL.\n",
    "\n",
    "The third, process based parallelism, can be done in python as it creates multiple interpreters so the GIL is not a problem.  In this case we make multiple separate instances of your code which run independently and can communicate with each other.  This is the case we will look at here with the package `mpi4py` which wraps the industry standard Message Passing Interface (MPI) used in high performance computing. Please note that the version of MPI that the mpi4py package was compiled against should be the same as the one used to run your MPI python code, otherwise it will fail. You can (probably) trust your cluster admin to sort that out, but this issue has a high chance to come up if you decide to run everything on your own machine, especially when installing things through a package manager. \n",
    "\n",
    "MPI is arguably the hardest so if you can master it then the rest should be easy(-er) to pick up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPI in python\n",
    "\n",
    "First we should note that parallel computing is hard.  You need to think carefully about what you are doing as parallel programmes can be difficult to debug as most debuggers can't track multiple processors.  Let's begin with a classic analogy to get us used to the idea of parallel computing, painting a heptagonal wall with a lovely rainbow pattern.\n",
    "\n",
    "You have:\n",
    "1. 7 painters who are ready to paint\n",
    "2. 7 pots of paint, one in each colour of the rainbow\n",
    "3. 7 walls that need to be painted with 7 stripes\n",
    "\n",
    "What is the best way to co-ordinate this?  For the analogy to work we have to assume the painters are fixed as they are the CPU's and in high performance computing \"take the back off the computer and move the chips around\" is seldom the best approach.  It is fairly obvious that we should get each painter to paint a stripe in one colour then pass the paint to the right.  When they receive the paint from the left they can then paint the next stripe.  If they do this 6 times the wall is finished.\n",
    "\n",
    "Now we can translate this to MPI speak.\n",
    "1. Each painter is a `rank` which controls one process\n",
    "2. Each pot of paint represents a data process\n",
    "3. Each wall represents a block of data.\n",
    "4. Passing the paint is a `point to point message` either a `send` or a `receive`\n",
    "\n",
    "The main point is that it would be ridiculous to keep the paint and to try to pass the walls instead.  Similarly the goal of parallelisation is to complete the process with the smallest amount of communication possible.  This is because communication is usually expensive, somewhere between reading from memory and reading from disk (the one it's closer to is very system dependent). As such, 'distributed' usually applies to the largest data objects. \n",
    "\n",
    "Note that since your data objects are probably going to be rather abstract arrays, it helps immensely to plan your MPI code with pen and paper first, drawing some type of scheme for which cells you are going to pass around between processes before you actually start typing.  \n",
    "\n",
    "### Basic commands\n",
    "\n",
    "\n",
    "Let's try some simple examples to see what MPI commands look like:\n",
    "\n",
    "**Example 1:**\n",
    "Here we just sent a message to each of the other ranks to say hello using `send` and `recv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Contents of Code/mpitest1.py (example x will be in file mpitest'x'.py)\n",
    "# Run this with: mpiexec -n 8 python mpitest1.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "# Set up communication stuff\n",
    "comm = MPI.COMM_WORLD\n",
    "\n",
    "# Find out which rank I am\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "# Find out how many ranks there are\n",
    "size = comm.Get_size()\n",
    "\n",
    "print( \"[{}] Starting\".format(rank) )\n",
    "\n",
    "# Send a message to all other ranks\n",
    "for i in range(1,size):\n",
    "#   Decide who I will send the message to\n",
    "#   And who my message will come from\n",
    "    rank_send = (rank+i)%size\n",
    "    rank_recv = (rank-i)%size\n",
    "#   Send the message, tag is not required here but is usefull later\n",
    "    comm.send(\"Hello from {}\".format(rank), dest=rank_send, tag=11)\n",
    "#   collect my message\n",
    "    message = comm.recv(source=rank_recv, tag=11)\n",
    "#   Print what happened\n",
    "    print(\"[{}] sent:{} recv:{} Message: {}\".format(rank,rank_send,rank_recv,message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tag helps you keep track when you receive multiple messages eg: I could send 5 messages from 0 to 1 and tag would tell me which was which.\n",
    "\n",
    "**Example 2:**\n",
    "Send an array or buffer type object to the other ranks.  Note here we must use `Send` and `Recv` not `send` and `recv`.  This capitalisation difference exists for all commands.  From here on we will only demonstrate one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send an array (or buffer like object) to all other ranks\n",
    "data_s = np.random.randint(1,9,(4))\n",
    "data_r = np.zeros((4), dtype='int')\n",
    "\n",
    "print( \"[{}] Starting with array \".format(rank), data_s )\n",
    "\n",
    "for i in range(1,size):\n",
    "#   Decide who I will send the message to\n",
    "#   And who my message will come from\n",
    "    rank_send = (rank+i)%size\n",
    "    rank_recv = (rank-i)%size\n",
    "#   Send the message, note capital \"S\"\n",
    "    comm.Send(data_s, dest=rank_send, tag=11)\n",
    "#   collect my message, note capital \"R\"\n",
    "    comm.Recv(data_r, source=rank_recv, tag=11)\n",
    "#   Print what happened\n",
    "    print(\"[{}] sent:{} recv:{} Array: \".format(rank,rank_send,rank_recv),data_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3:**\n",
    "This type of operation where you are passing information around the ranks is best done with `sendrecv` which combines the two (we will discuss why later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send an array (or buffer like object) to all other ranks\n",
    "data_s = np.random.randint(1,9,(4))\n",
    "data_r = np.zeros((4), dtype='int')\n",
    "\n",
    "print( \"[{}] Starting with array \".format(rank), data_s )\n",
    "\n",
    "for i in range(1,size):\n",
    "    #   Decide who I will send the message to\n",
    "    #   And who my message will come from\n",
    "    rank_send = (rank+i)%size\n",
    "    rank_recv = (rank-i)%size\n",
    "    #   Send and recieves messages simultaneously\n",
    "    comm.Sendrecv(data_s, dest=rank_send, recvbuf=data_r, source=rank_recv)\n",
    "    #   Print what happened\n",
    "    print(\"[{}] sent:{} recv:{} Array: \".format(rank,rank_send,rank_recv),data_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4:**\n",
    "Send data from one rank to all others using a broadcast `bcast` (`Bcast` works the same here except `None` becomes an empty array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initilise the data only on rank 0\n",
    "if rank==0:\n",
    "    data = \"0 is the best rank\"\n",
    "else:\n",
    "    data = None\n",
    "\n",
    "#Send the data to all other ranks\n",
    "data = comm.bcast(data, root=0)\n",
    "\n",
    "print(\"[{}] I just heard that {}\".format(rank,data) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 5:**\n",
    "More usefully we can break a data object up and send a part to each rank with `Scatter` or put it back together with `Gather`.  There is also `Allgather` for a `Gather` +`Bcast`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data\n",
    "data_r = np.empty(5,dtype='int')\n",
    "\n",
    "if rank==0:\n",
    "    data_s = np.arange(5*size)\n",
    "else:\n",
    "    data_s = None\n",
    "\n",
    "# Scattering\n",
    "comm.Scatter(data_s,data_r,root=0)\n",
    "\n",
    "print(\"[{}] My data is \".format(rank),data_r)\n",
    "\n",
    "if rank==3:\n",
    "    data_s = np.empty(5*size, dtype='int')\n",
    "# Gather it back up\n",
    "comm.Gather(data_r,data_s,root=3)\n",
    "\n",
    "print(\"[{}] My data now is \".format(rank),data_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also `alltoall` which combines both\n",
    "\n",
    "**Example 6:**\n",
    "The most useful (in my opinion anyway) is the global reduce which takes buffers form each rank then combines them with an operation like: `MIN`, `MAX`, `SUM`, `PROD`,...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data\n",
    "data_s = np.random.randint(1,9,(5))\n",
    "data_r = np.empty(5,dtype='int')\n",
    "\n",
    "print( \"[{}] Starting with data: \".format(rank),data_s )\n",
    "\n",
    "comm.Reduce(data_s,data_r,op=MPI.SUM,root=0)\n",
    "\n",
    "if rank==0:\n",
    "    print(\"summed data: \",data_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have `Allreduce` which is a `Reduce` combined with a `Bcast`.  One other usefull command is `Barrier` which has no arguments and simply holds all processes until all reach it so forces syncronisation.  Don't be tempted to use this unless you have to as it just slows down your code.\n",
    "\n",
    "All the above commands are blocking commands in that the process doesn't move on until they are complete (in theory).  This means that we always have to wait until comunications have completed before we can continue with our code.  Communication is usually slow so there are also non-blocking versions of `send` and `recv` we can use.  This allows us to send information then continue calculation while we wait for it to arrive.  The non-blocking versions are called `Isend` and `Irecv` and work like this:\n",
    "\n",
    "**Example 7:**\n",
    "Use non-blocking commands so we can do stuff while we wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send an array (or buffer like object) to all other ranks\n",
    "data_s = np.random.randint(1,9,(200))\n",
    "data_r = np.zeros((200), dtype='int')\n",
    "\n",
    "\n",
    "for i in range(1,size):\n",
    "#   Decide who I will send the message to\n",
    "#   And who my message will come from\n",
    "    rank_send = (rank+i)%size\n",
    "    rank_recv = (rank-i)%size\n",
    "#   Send the message, note capital \"S\"\n",
    "\n",
    "#   Create recieve\n",
    "    request = comm.Irecv(data_r, source=rank_recv, tag=11)\n",
    "#   Create request\n",
    "    comm.Isend(data_s, dest=rank_send, tag=11)\n",
    "    \n",
    "#   Count while I wait for message to arrive\n",
    "    i=0\n",
    "    while not request.Get_status():\n",
    "        i+=1\n",
    "\n",
    "#   collect my message, note capital \"R\"\n",
    "#   Print what happened\n",
    "print(\"[{}] while waiting I counted to \".format(rank),i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deadlocks\n",
    "\n",
    "One key thing to watch our for when using MPI is deadlocks.  This is when you have one rank stuck waiting for a message that another rank can't send.  This usually happens when you get you send\\recv out of order which is easier to do than you think as you never know the order the ranks will meet the code.  It is possible you already met this as some of the examples are not safe.  The first and second example can both deadlock if they happen to have large data that they can't buffer.  \n",
    "\n",
    "What happens if all ranks enter the `send` at the same time? As the sends are blocking sends they won't complete until the data is received by `recv`. But as all ranks are held in the `send` command, none make it to the `recv` and the code stalls forever.  Here this is unlikely as the message is short so it is sent to buffer allowing the send to move onto the receive.  This is the danger with parallel programming.  Small tests like this can be fine but if I then ported the code to a large machine with slow communication and I was sending large amounts of data this may happen most of the time. Then when I move back to a small example to see what was going on it would work fine.  This is why we have `sendrecv` which both sends and waits for data so can't deadlock. This solution forces the code to synchronise which can slow the code down.  The alternative is to use non-blocking communication which is more efficient but you have to check the messages complete before you try to use the data sent.  Collectives can also block if not all ranks can get to them.  For example this code which to new people sort of looks sensible will stall forever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_r = np.empty(5,dtype='int')\n",
    "\n",
    "if rank==0:\n",
    "    data_s = np.arange(5*size)\n",
    "else:\n",
    "    data_s = None\n",
    "\n",
    "if rank==0:\n",
    "    comm.Scatter(data_s,data_r,root=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best idea is to try to use collectives wherever possible, then non-blocking, then blocking.\n",
    "\n",
    "You should note that collectives can be either blocking or non-blocking depending on the implementation so don't rely on them to sync your ranks.  Also make sure all ranks see the same collectives in the same order as this may deadlock the code too (basically don't use them in if statements like above)\n",
    "\n",
    "## Parallel patterns\n",
    "\n",
    "There are lots of different approaches to parallelising code.  The best one for your code is quite problem specific.  Here we will look as some basic \"patterns\" that are commonly used.  First there are a few things to consider.\n",
    "\n",
    "- How much of my code can be parallelised? If 30% of your code is serial then the maximum speedup you can achieve is $1/0.3 = 3.3$x.  This gives you a benchmark to work against and whether the effort will be worth it. This is where profiling is important.\n",
    "\n",
    "- How will my code scale?  If you plan to use you code for large problems but only use a small test data set you need to check how your approach will work on the full solution. Suppose you have two approaches, the first is faster on the small problem but scales as $O(N^2)$ (eg gaussian smoothing in 2D), the second is slower but scales as $O(N\\ln(N))$ (eg gaussian smoothing via fft then weighting modes then ifft back).  Here the first would be faster for small problems but the second would be best for the large problem.\n",
    "\n",
    "- Load balance.  How will you ensure that each CPU has the roughly same amount of work to do?\n",
    "\n",
    "Parallelisation also has costs\n",
    "\n",
    "- Efficiency (Sometimes we will chose algorithms that parallelise well but are slower for serial. Also communication can have significant costs)\n",
    "- Simplicity (Code gets harder to read and maintain as we will see)\n",
    "- Portability (parallel code is often written with a particular system in mind so it can be hard to migrate to new machines)\n",
    "\n",
    "\n",
    "With this lets look at some simple parallelisation patterns (in order of complexity)\n",
    "\n",
    "1. Task parallelisim\n",
    "2. Domain decomposition\n",
    "3. Divide and conquer\n",
    "4. Recursive data\n",
    "5. Pipelines\n",
    "\n",
    "### Task parallelism\n",
    "\n",
    "Here we have a list of independent tasks that need to be completed and we just divide them up over the ranks.  The best example is splitting a loop like in the following example:\n",
    "\n",
    "**Pattern 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contents of Code/mpipattern1.py (pattern x will be in file mpipattern'x'.py)\n",
    "# Run this with: mpiexec -n 8 python mpipattern1.py\n",
    "\n",
    "\"\"\"\n",
    "This routine sums the numbers from 1 to loops\n",
    "using MPI to parallelise the calculation\n",
    "\"\"\"\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "\n",
    "def divide_loops(loops,size):\n",
    "    \"\"\"\n",
    "    Divides 'loops' into 'size' groups\n",
    "    \n",
    "    Divide the number of loops into as close to equal\n",
    "    size chunks as possible for use with parallelisation\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    loops: int\n",
    "        the number of loops to divide up\n",
    "    size: int\n",
    "        the number of groups to break loops into\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    start_loop: int\n",
    "        The loop number to start from\n",
    "    end_loop: int\n",
    "        The loop number to stop at \n",
    "    \"\"\"\n",
    "    # floor division\n",
    "    loop_rank=loops//size\n",
    "    # remainder\n",
    "    auxloop = loops%size\n",
    "    #calculate start and end\n",
    "    start_loop = rank*loop_rank\n",
    "    end_loop = (rank+1)*loop_rank\n",
    "    \n",
    "    # allocate remainder across loops\n",
    "    if(auxloop!=0):\n",
    "        if (rank < auxloop):\n",
    "            start_loop = start_loop + rank\n",
    "            end_loop = end_loop + rank + 1\n",
    "        else:\n",
    "            start_loop = start_loop + auxloop\n",
    "            end_loop = end_loop + auxloop\n",
    "    # return start and end\n",
    "    return start_loop, end_loop\n",
    "\n",
    "# initilise MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "print( \"[{}] Starting\".format(rank) )\n",
    "\n",
    "# number to sum to\n",
    "loops = 1000\n",
    "\n",
    "# divide number across processors\n",
    "s,e = divide_loops(loops,size)\n",
    "\n",
    "# perform partial sums for each section\n",
    "x=0\n",
    "for i in range(s,e):\n",
    "    x += i+1\n",
    "\n",
    "print( \"[{}] Local sum {}\".format(rank,x) )\n",
    "\n",
    "# combine results using reduce\n",
    "y = comm.reduce(x,op=MPI.SUM,root=0)\n",
    "\n",
    "# print output\n",
    "if rank==0:\n",
    "    print( \"[{}] Total sum {}\".format(rank,y) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multiple loops it is usually best to flatten them.  An example is looping over $i,j$ where $i<=j$.  Here splitting on $i$ would be bad as the ranks would get different amounts of work to do affecting load balance. Instead flatten them into a single loop. So to calculate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imax = 10\n",
    "x=0\n",
    "for ii in range(imax):\n",
    "    for jj in range(ii,imax):\n",
    "        x += ii+jj\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could do:\n",
    "\n",
    "**Pattern 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This routine sum the triangular double sum of numbers from 1 to imax\n",
    "using MPI to parallelise the calculation\n",
    "\n",
    "To ensure load balancing we flatten the loops\n",
    "\"\"\"\n",
    "from mpi4py import MPI\n",
    "\n",
    "def divide_loops(loops,size):\n",
    "    \"\"\"\n",
    "    Divides 'loops' into 'size' groups\n",
    "    \n",
    "    Divide the number of loops into as close to equal\n",
    "    size chunks as possible for use with parallelisation\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    loops: int\n",
    "        the number of loops to divide up\n",
    "    size: int\n",
    "        the number of groups to break loops into\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    start_loop: int\n",
    "        The loop number to start from\n",
    "    end_loop: int\n",
    "        The loop number to stop at \n",
    "    \"\"\"\n",
    "    # floor division\n",
    "    loop_rank=loops//size\n",
    "    # remainder\n",
    "    auxloop = loops%size\n",
    "    #calculate start and end\n",
    "    start_loop = rank*loop_rank\n",
    "    end_loop = (rank+1)*loop_rank\n",
    "    \n",
    "    # allocate remainder across loops\n",
    "    if(auxloop!=0):\n",
    "        if (rank < auxloop):\n",
    "            start_loop = start_loop + rank\n",
    "            end_loop = end_loop + rank + 1\n",
    "        else:\n",
    "            start_loop = start_loop + auxloop\n",
    "            end_loop = end_loop + auxloop\n",
    "    # return start and end\n",
    "    return start_loop, end_loop\n",
    "\n",
    "def gen(s,e,imax):\n",
    "    \"\"\"\n",
    "    Create generator for part of triangular sum\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    s: int\n",
    "        the loop number to begin at\n",
    "    e: int\n",
    "        the loop number to end at\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    None: \n",
    "    \n",
    "    \"\"\"\n",
    "    # initilise count\n",
    "    count = 1\n",
    "    #triangular loop\n",
    "    for ii in range(imax):\n",
    "        for jj in range(ii,imax):\n",
    "            # gone to far then end\n",
    "            if count>e:\n",
    "                break\n",
    "            # Don't return pair until we have gone s loops\n",
    "            if count>s:\n",
    "                yield (ii,jj)\n",
    "            count += 1\n",
    "\n",
    "# initilise MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "print( \"[{}] Starting\".format(rank) )\n",
    "\n",
    "# double triangular sum numbers from 1 to imax\n",
    "imax = 100\n",
    "loops = imax*(imax+1)//2\n",
    "\n",
    "# divide up the loops\n",
    "s,e = divide_loops(loops,size)\n",
    "# create generator for these loops\n",
    "G1 = gen(s,e,imax)\n",
    "\n",
    "# compute double sum\n",
    "x=0\n",
    "for item in G1:\n",
    "    x += item[0]+item[1]\n",
    "\n",
    "print( \"[{}] Local sum {}\".format(rank,x) )\n",
    "\n",
    "# combine results using reduce\n",
    "y = comm.reduce(x,op=MPI.SUM,root=0)\n",
    "\n",
    "# print output\n",
    "if rank==0:\n",
    "    print( \"[{}] Total sum {}\".format(rank,y) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For very large jobs with extremely uneven workloads we could instead opt of a master-slave set up where the jobs are distributed by a master process with all the work being done on slaves.   This can be quite tricky to programme but here is a simple (relative to what it could look like) example that calculates $n(n+1)/2$ for $n$ from 0 to 10:\n",
    "\n",
    "**Pattern 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a master slave MPI code for calculating n(n+1)/2 for a set of n\n",
    "The communication code is quite general however\n",
    "\"\"\"\n",
    "from mpi4py import MPI\n",
    "\n",
    "# 0 is master\n",
    "# rest are slaves\n",
    "\n",
    "# intiilise MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# Master rank section\n",
    "if rank==0:\n",
    "    # define list of tasks\n",
    "    tasklist = (i for i in range(10))\n",
    "    \n",
    "    #set count of finished ranks\n",
    "    finished = 0\n",
    "    \n",
    "    #create status object for MPI_probe\n",
    "    status = MPI.Status()\n",
    "    \n",
    "    #initial allocation of jobs\n",
    "    for i in range(1,size):\n",
    "        # check there are enough jobs for the number of ranks\n",
    "        # -1 is a flag to say \"no more work\" \n",
    "        try:\n",
    "            message = next(tasklist)\n",
    "        except StopIteration:\n",
    "            message = -1\n",
    "        \n",
    "        # now we send initial job ID's to slaves\n",
    "        print(\"[{}] Sending task: {} to rank {}\".format(rank,message,i))\n",
    "        comm.isend(message, dest=i, tag=i)\n",
    "\n",
    "    # Now we check for messages from complete jobs\n",
    "    # then allocate new jobs to the slaves\n",
    "    while finished<size-1:\n",
    "        # Check for waiting messages with probe \n",
    "        flag = comm.iprobe(status=status, source=MPI.ANY_SOURCE, tag=MPI.ANY_TAG)\n",
    "        \n",
    "        # if a message is waiting then enter this loop to recieve it\n",
    "        if flag==True:\n",
    "            # status object stores the origin of the message (and tag etc..)\n",
    "            source = status.source\n",
    "            #recv the message\n",
    "            test = comm.irecv(source=source, tag=source)\n",
    "            reply = test.wait()\n",
    "            print(\"[{}] Recieving result: {} from rank {}\".format(rank,reply,source))\n",
    "            \n",
    "            # now check the reply, -1 means the slave receieved a -1 and it's letting\n",
    "            # us know that it's finished so we add it to our finshed count\n",
    "            # otherwise we send it its next job ID (which may be -1 if there are \n",
    "            # no more jobs)\n",
    "            if reply==-1:\n",
    "                finished +=1\n",
    "                print(\"[{}] Recieved termination finished count: {}\".format(rank,finished))\n",
    "            else:\n",
    "                print(\"[{}] Done with result {}\".format(rank,reply))\n",
    "                try:\n",
    "                    message = next(tasklist)\n",
    "                except StopIteration:\n",
    "                    message = -1\n",
    "                print(\"[{}] Sending task: {} to rank {}\".format(rank,message,i))\n",
    "                comm.isend(message, dest=source, tag=source)\n",
    "\n",
    "# Slave ranks section\n",
    "else:\n",
    "    # this loop keeps us checking for jobs until we recieve a -1\n",
    "    while True:\n",
    "        # recvout next job ID\n",
    "        test = comm.irecv(source=0, tag=rank)\n",
    "        task = test.wait()\n",
    "        print(\"[{}] Recieving task: {} from rank {}\".format(rank,task,0))\n",
    "        \n",
    "        # is job ID = -1 then no more jobs and we can stop\n",
    "        # We let the master know we are stopping by sending -1 back\n",
    "        # Otherwise we do the job associated with the ID we recieve\n",
    "        if task==-1:\n",
    "            comm.isend(-1, dest=0, tag=rank)\n",
    "            print(\"[{}] Sending termination to rank {}\".format(rank,0))\n",
    "            break\n",
    "        else:\n",
    "            # This single line is the actual job\n",
    "            result = task*(task+1)//2\n",
    "            \n",
    "            # now we send our result back to the master\n",
    "            comm.isend(result, dest=0, tag=rank)\n",
    "            print(\"[{}] Sending result {} to rank {}\".format(rank,result,0))\n",
    "\n",
    "# Finished message\n",
    "print(\"[{}] I'm done\".format(rank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Decomposition\n",
    "\n",
    "Task decomposition distributes the tasks.  Here we will instead distribute the data. A simple example is multiplication of a $n\\times m$ matrix $M$ by a $n\\times n$ matrix $N$ where $m>>n$.  We split the matrix M up over processors then apply the dot product then gather. This would look like:\n",
    "\n",
    "**Pattern 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A very simple domain decomposition code to multiple matricies in parallel\n",
    "ie: M(mm,5) . N(5,5) in parallel using MPI\n",
    "\"\"\"\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# The long matrix dimension\n",
    "mm = 100\n",
    "# divide this by number of processors\n",
    "# we have made no allowances for remainder\n",
    "# in a prefect world we would keep track of this\n",
    "# and pad our matrixies accordingly\n",
    "split = mm//size\n",
    " \n",
    "# Object to recv matrix from scatter\n",
    "matrix_M_r = np.empty((split,5),dtype='int')\n",
    "\n",
    "# matric to multiply the large one by\n",
    "matrix_N = np.arange(1,26).reshape((5,5))\n",
    "\n",
    "# Initilise the large matrix on rank 0\n",
    "if rank==0:\n",
    "    matrix_M_s = np.array([range(i,i+5) for i in range(mm)])\n",
    "else:\n",
    "    matrix_M_s = None\n",
    "    \n",
    "if rank==0:\n",
    "    print(\"[{}] My send data is \".format(rank),matrix_M_s)\n",
    "\n",
    "# Scatter the large matric to all ranks\n",
    "comm.Scatter(matrix_M_s,matrix_M_r,root=0)\n",
    "\n",
    "print(\"[{}] My recv data is: \".format(rank),matrix_M_r)\n",
    "\n",
    "# compute multiplication\n",
    "matrix_M_r = np.dot(matrix_M_r,matrix_N)\n",
    "\n",
    "# gather reults back together\n",
    "comm.Gather(matrix_M_r,matrix_M_s,root=0)\n",
    "    \n",
    "if rank==0:\n",
    "    print(\"[{}] My gath data is \\n\".format(rank),matrix_M_s)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More usually this is used for large simulations and so we need the different partitions to communicate with each other.  Also we should note that there are many different ways to partition the data.  We can divide up the domain the simulation runs on, either into strips or squares (2D) or planes, columns or cubes (3D).  For nbody type codes where we are tracking particles this could lead to load imbalances when particles congregate in specific cells.  Here we could either have an adaptive mesh or we could partition the particles instead.  Our example for 1D domain decomposition with communication is left as an exercise using our \"game of life\" code.  Domain decomposition is sufficiently common MPI has specific commands for dealing with this under the heading of Topologies.  There are two main classes: Cartesian and Graph.  This is getting quite advanced so I'm not going to get into general examples but here are what the basic commands look like for future reference:\n",
    "\n",
    "**Pattern 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cartesian topology\n",
    "\"\"\"\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "if size==8:\n",
    "    cartesian3d = comm.Create_cart(dims = [2,2,2],periods = [True,True,True],reorder=True)\n",
    "\n",
    "    coord3d = cartesian3d.Get_coords(rank)\n",
    "    print (\"In 3D topology, Processor \",rank, \" has coordinates \",coord3d)\n",
    "\n",
    "    left,right = cartesian3d.Shift(direction = 0,disp=1)\n",
    "    up,down = cartesian3d.Shift(direction = 1,disp=1)\n",
    "    back,front = cartesian3d.Shift(direction = 2,disp=1)\n",
    "\n",
    "    print (\"In 3D topology, Processor \",rank, \" has neighbours \", left,\" and \", right)\n",
    "    print (\"In 3D topology, Processor \",rank, \" has neighbours \", up,\" and \", down)\n",
    "    print (\"In 3D topology, Processor \",rank, \" has neighbours \", back,\" and \", front)\n",
    "\n",
    "#     We can also make sub-topologies so we only communicate to a small section of ranks\n",
    "    cartesian2d = cartesian3d.Sub(remain_dims=[False,True,True])\n",
    "    rank2d = cartesian2d.Get_rank()\n",
    "    coord2d = cartesian2d.Get_coords(rank2d)\n",
    "\n",
    "    print (\"In 2D topology, Processor \",rank,\" has new rank \",rank2d,\" and coordinates \", coord2d)\n",
    "    \n",
    "    left,right = cartesian2d.Shift(direction = 0,disp=1)\n",
    "    up,down = cartesian2d.Shift(direction = 1,disp=1)\n",
    "\n",
    "    print (\"In 2D topology, Processor \",rank, \" has new rank \",rank2d,\" and neighbours \", left,\" and \", right)\n",
    "    print (\"In 2D topology, Processor \",rank, \" has new rank \",rank2d,\" and neighbours \", up,\" and \", down)\n",
    "    \n",
    "    \n",
    "# communication should be done using non-blocking send and receives\n",
    "# rather than collectives as MPI can reorder process labels to \n",
    "# optimise communication.  However we should have the following collectives\n",
    "# for neighbours (as they exist in the github: https://github.com/erdc/mpi4py/blob/master/src/MPI/Comm.pyx):\n",
    "\n",
    "# output = cartesian3d.neighbor_allgather(sendobj=rank)\n",
    "# output = cartesian3d.neighbor_alltoall(sendobj=rank)\n",
    "\n",
    "# But they currently give: NotImplementedError it doesn't exist for my MPI implementation..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pattern 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "General Graph topology\n",
    "\"\"\"\n",
    "from mpi4py import MPI\n",
    "import random\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "index, edges = [0], []\n",
    "for i in range(size):\n",
    "    pos = index[-1]\n",
    "    connections = random.randint(1,5)\n",
    "    index.append(pos+connections)\n",
    "    edges1=[]\n",
    "    for j in range(connections):\n",
    "        x = random.randint(0,size-1)\n",
    "        while x==rank or x in edges1:\n",
    "            x = random.randint(0,size-1)\n",
    "        edges1.append(x)\n",
    "        \n",
    "    edges += edges1\n",
    "\n",
    "graph = comm.Create_graph(index[1:], edges)\n",
    "\n",
    "neighs = graph.Get_neighbors(rank)\n",
    "\n",
    "print (\"In Graph we assigned processor \",rank, \" to have neighbours \", neighs)\n",
    "\n",
    "if rank==0:\n",
    "    print (\"Graph has dimensions \",graph.dims)\n",
    "    print (\"Graph has #nodes \",graph.nnodes)\n",
    "    print (\"Graph has #edges \",graph.nedges)\n",
    "    print (\"Graph has index \",graph.index)\n",
    "    print (\"Graph has edges \",graph.edges)\n",
    "\n",
    "print (\"Processor \",rank,\" has in degree \",graph.indegree)\n",
    "print (\"Processor \",rank,\" has out degree \",graph.outdegree)\n",
    "print (\"Processor \",rank,\" has in edges \",graph.inedges)\n",
    "print (\"Processor \",rank,\" has out edges \",graph.outedges)\n",
    "\n",
    "# We should be able to only specify connection to nodes,\n",
    "# of which we can have several, on this rank with the following:\n",
    "\n",
    "# sources = [rank]\n",
    "# connections = random.randint(1,5)\n",
    "# degrees = [connections]\n",
    "# destinations = []\n",
    "# for i in range (connections):\n",
    "#     x = random.randint(0,size-1)\n",
    "#     while x==rank:\n",
    "#         x = random.randint(0,size-1)\n",
    "#     destinations.append(x)\n",
    "#\n",
    "# graph = comm.Create_dist_graph(sources,degrees,destinations)\n",
    "\n",
    "# But they currently give: NotImplementedError so it doesn't exist for my MPI implementation..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide and Conquer\n",
    "\n",
    "This technique is used when dividing or recombining tasks/data is non trivial so we instead scatter/gather iteratively.  So our algorithm looks like this:\n",
    "\n",
    "![](Plots/DivideConquer.png)\n",
    "\n",
    "A good example is sorting a very long list.  We can divide up the elements easily and sort then but merging the data back is difficult.  Instead here we merge pairs of lists back together and slowly recombine the full list\n",
    "\n",
    "**Pattern 7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code to compute a 'merge-sort' using MPI\n",
    "\"\"\"\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# calculate how many merge loops we will need\n",
    "# we have assumed that the number or processors is a power of 2\n",
    "# otherwise we need to have lots of logic for edge cases\n",
    "# in our merge tree\n",
    "levels = int(math.log2(size))\n",
    "\n",
    "def mergelist(list1,list2):\n",
    "    \"\"\"\n",
    "    Merges two sorted list into one sorted list\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    list1: numpy array\n",
    "        our first sorted list\n",
    "    list2: numpy array\n",
    "        our second sorted list\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    list3: numpy array\n",
    "        our merged sorted list\n",
    "    \"\"\"\n",
    "    # get sizes (len works on 1d arrays)\n",
    "    size1 = len(list1)\n",
    "    size2 = len(list2)\n",
    "    size3 = size1+size2\n",
    "    # create blank list\n",
    "    list3 = np.empty((size3),dtype='int')\n",
    "    # i counts list 1, j list 2 and k list3\n",
    "    i=0\n",
    "    j=0\n",
    "    k=0\n",
    "    #loop over both lists until we reach the end of one\n",
    "    while i<size1 and j<size2:\n",
    "        # test which is smaller and add it to list3\n",
    "        # and increment the counter for that list and list3\n",
    "        if list1[i]<list2[j]:\n",
    "            list3[k] = list1[i]\n",
    "            i+=1\n",
    "        else:\n",
    "            list3[k] = list2[j]\n",
    "            j+=1\n",
    "        k+=1\n",
    "    \n",
    "    # add remaining elements for the list\n",
    "    # we didn't get to the end of\n",
    "    while i<size1:\n",
    "        list3[k] = list1[i]\n",
    "        i+=1\n",
    "        k+=1\n",
    "        \n",
    "    while j<size2:\n",
    "        list3[k] = list2[j]\n",
    "        j+=1\n",
    "        k+=1\n",
    "    \n",
    "    return list3\n",
    " \n",
    "# set list length\n",
    "length = 200\n",
    "# divide this by number of processors\n",
    "# we have made no allowances for remainder\n",
    "# in a prefect world we would keeep track of this\n",
    "# and pad our arrays accordingly\n",
    "split = length//size\n",
    "\n",
    "# create list to recv data from scatter\n",
    "list1 = np.empty((split),dtype='int')\n",
    "\n",
    "# create long list on rank 0\n",
    "if rank==0:\n",
    "    listall = np.random.randint(0,100,(200))\n",
    "else:\n",
    "    listall = None\n",
    "    \n",
    "if rank==0:\n",
    "    print(\"[{}] My send data is \".format(rank),listall)\n",
    "\n",
    "#scatter orginal list\n",
    "comm.Scatter(listall,list1,root=0)\n",
    "\n",
    "# sort our section\n",
    "list1 = np.sort(list1)\n",
    "\n",
    "level = 2\n",
    "# now merge back.  This is a bit fiddly\n",
    "# we first send from 1mod2 to 0mod2 then from\n",
    "# 2mod4 to 0mod4  and so on merging at each step\n",
    "for i in range(1,levels+1):\n",
    "    if rank%2**i == 2**(i-1):\n",
    "        dest = rank - 2**(i-1)\n",
    "        comm.Send(list1, dest=dest, tag=11)\n",
    "    elif rank%2**i == 0:\n",
    "        list2 = np.empty((split*2**(i-1)),dtype='int')\n",
    "        src = rank + 2**(i-1)\n",
    "        comm.Recv(list2, source=src, tag=11)\n",
    "        list1 = mergelist(list1,list2)\n",
    "\n",
    "#print answer\n",
    "if rank==0:\n",
    "    print(list1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive data\n",
    "\n",
    "Sometimes we will want to perform recursive operations on data which are inherently serial.  In these situations we can expose parallelism by using recasting the problem to expose concurrency.  This can be difficult and often the algorithm is slower but with enough cores the code will still run faster.  A simple example is calculating a cumulative sum for a list.  Here we would normally start at the beginning and compute the sum as we progress along the list.  This is an $O(N)$ operation but is serial so can only use one core.  We can instead recast the problem by calculating recursive partial sums like so:\n",
    "\n",
    "![](Plots/Recursive.png)\n",
    "\n",
    "Now the algorithm is $O(N\\ln(N))$ but each element is calculated separately. Now we have to be careful as if we have fewer than $O(\\ln(N))$ processors/threads then the code will be lower.  This could still be useful if the list is very large so can't fit in memory. Here is an example of how this might look (This ends up being a divide and conquer algorithm like the previous example):\n",
    "\n",
    "**Pattern 8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code to compute a cumulitive sum using MPI after reacting the problem\n",
    "\"\"\"\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# initilise\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# calculate how many send loops we will need\n",
    "# we have assumed that the number or processors is a power of 2\n",
    "# otherwise we need to have lots of logic for edge cases\n",
    "# in our merge tree\n",
    "levels = int(math.log2(size))\n",
    "\n",
    "# set list length\n",
    "length = 200\n",
    "# divide this by number of processors\n",
    "# we have made no allowances for remainder\n",
    "# in a prefect world we would keeep track of this\n",
    "# and pad our arrays accordingly\n",
    "split = length//size\n",
    "\n",
    "# create list to recv data from scatter\n",
    "list1 = np.empty((split),dtype='int')\n",
    "\n",
    "# create long list on rank 0\n",
    "if rank==0:\n",
    "    listall = np.random.randint(0,100,(length))\n",
    "else:\n",
    "    listall = None\n",
    "    \n",
    "if rank==0:\n",
    "    print(\"[{}] My send data is \".format(rank),listall)\n",
    "\n",
    "#scatter orginal list   \n",
    "comm.Scatter(listall,list1,root=0)\n",
    "\n",
    "# compute cumulitive sum for our section\n",
    "x=0\n",
    "for index, item in enumerate(list1):\n",
    "    x += item\n",
    "    list1[index] = x\n",
    "\n",
    "# now send cum sum to other ranks.  This is a bit fiddly\n",
    "# we first send from 1mod2 to 0mod2 then from\n",
    "# 2mod4 to 0mod4  and so on adding the new section on at each setp    \n",
    "for i in range(1,levels+1):\n",
    "    if rank%2**i == 0:\n",
    "        dest = rank + 2**(i-1)\n",
    "        comm.send(list1[-1], dest=dest, tag=11)\n",
    "    elif rank%2**i == 2**(i-1):\n",
    "        src = rank - 2**(i-1)\n",
    "        x = comm.recv(source=src, tag=11)\n",
    "        list1 = list1+x\n",
    "\n",
    "# gather the cumultive sums back together\n",
    "comm.Gather(list1,listall,root=0)\n",
    "\n",
    "# output result\n",
    "if rank==0:\n",
    "    print(listall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "\n",
    "The final pattern we will meet is pipelines here the data flows through the ranks with different operations being applied at each stage.  This can work well for heavy compute algorithms or ones that use special chips like GPU's.  You may want to input a stream of vectors to be multiplied by several matrixes with different operations inbetween so rather than moving the matrixes you pass the vectors along a production line where the matrixes remain fixed.  This is just like a production line in a car plant for example.  Here the communication is just simple non-blocking send and recv's between the ranks.  I currently don't have a good example but I will try to find one and add it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "Whatever approach you chose for your code, it is important to always try to check how your final result scales to a larger number of processes on some kind of shorter problem before submitting a 12 hours long job taking up half the cluster only to find out that you didn't benefit at all from increasing the number of tasks from, say, 64 to 128. For example, below is a picture of scaling for one of the matrix multiplication algorithms. We see that, while the computation time scales almost perfectly, the communication time pretty much doesn't benefit from increasing the number of processes and, as a result, this algorithm on the given data set size probably shouldn't ask for more than 80 tasks, as it cannot use those extra resources effectively. You should be able to somewhat predict the scaling for your algorithm, but MPI can absolutely surprise you sometimes, not to mention your predictions might be miscalculated, so testing the scaling is quite a necessary part of your parallel code development.    \n",
    "\n",
    "![](Plots/scaling_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Task parallelism: Take the code `Solutions/mpi_triangle_int.py` and parallelise it to split the calculation of $E_{ijk}$ across 4 processes.\n",
    "\n",
    "\n",
    "2. Decomposing a domain: Parallelise the code `Solutions/mpi_GOL.py` to split the game of life across 4 ranks for a 10x40 game board.  Initial starting position is defined inside.  After 60 iterations the picture of a \"duck\" should be on the last tile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation for mpi4py isn't great but some information can be found here:\n",
    "\n",
    "https://info.gwdg.de/~ceulig/docs-dev/doku.php?id=en:services:application_services:high_performance_computing:mpi4py\n",
    "\n",
    "https://mpi4py.readthedocs.io/en/stable/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
